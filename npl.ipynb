{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "txt = \"\"\"\"Stop blushing. I'm not needling, really I'm not. Do you know, I\r\n",
    "had a dream an hour ago. I lay down for a cat-nap and in this dream\r\n",
    "you and I, Montag, got into a furious debate on books. You towered\r\n",
    "with rage, yelled quotes at me. I calmly parried every thrust. Power, I\r\n",
    "said, And you, quoting Dr. Johnson, said `Knowledge is more than\r\n",
    "equivalent to force!' And I said, `Well, Dr. Johnson also said, dear boy,\r\n",
    "that \"He is no wise man that will quit a certainty for an uncertainty.'\"\r\n",
    "Stick with the fireman, Montag. All else is dreary chaos!\"\r\n",
    "\"Don't listen,\" whispered Faber. \"He's trying to confuse. He's\r\n",
    "slippery. Watch out!\"\r\n",
    "Beatty chuckled. \"And you said, quoting, `Truth will come to light,\r\n",
    "murder will not be hid long!' And I cried in good humour, 'Oh God, he\r\n",
    "speaks only of his horse!' And `The Devil can cite Scripture for his\r\n",
    "purpose.' And you yelled, 'This age thinks better of a gilded fool, than\r\n",
    "of a threadbare saint in wisdom's school!' And I whispered gently, 'The\r\n",
    "dignity of truth is lost with much protesting.' And you screamed,\r\n",
    "'Carcasses bleed at the sight of the murderer!' And I said, patting your\r\n",
    "hand, 'What, do I give you trench mouth?' And you shrieked,\r\n",
    "'Knowledge is power!' and 'A dwarf on a giant's shoulders of the\r\n",
    "furthest of the two!' and I summed my side up with rare serenity in,\r\n",
    "'The folly of mistaking a metaphor for a proof, a torrent of verbiage for\r\n",
    "a spring of capital truths, and oneself as an oracle, is inborn in us, Mr.\r\n",
    "Valery once said.'\" \r\n",
    "\r\n",
    "I think you may like to know something of his person and character. He \r\n",
    "had an excellent constitution of body, was of middle stature, but well set, \r\n",
    "and very strong; he was ingenious, could draw prettily, was skilled a little \r\n",
    "in music, and had a clear, pleasing voice, so that when he played psalm tunes\r\n",
    " on his violin and sung withal, as he sometimes did in an evening after \r\n",
    "the business of the day was over, it was extremely agreeable to hear. He had \r\n",
    "a mechanical genius too, and, on occasion, was very handy in the use of other \r\n",
    "tradesmen's tools; but his great excellence lay in a sound understanding and \r\n",
    "solid judgment in prudential matters, both in private and publick affairs. \r\n",
    "In the latter, indeed, he was never employed, the numerous family he had to \r\n",
    "educate and the straitness of his circumstances keeping him close to his trade; \r\n",
    "but I remember well his being frequently visited by leading people, who \r\n",
    "consulted him for his opinion in affairs of the town or of the church he \r\n",
    "belonged to, and showed a good deal of respect for his judgment and advice: \r\n",
    "he was also much consulted by private persons about their affairs when any difficulty \r\n",
    "occurred, and frequently chosen an arbitrator between contending parties. \r\n",
    "At his table he liked to have, as often as he could, some sensible friend or \r\n",
    "neighbor to converse with, and always took care to start some ingenious or useful \r\n",
    "topic for discourse, which might tend to improve the minds of his children. \r\n",
    "By this means he turned our attention to what was good, just, and prudent in \r\n",
    "the conduct of life; and little or no notice was ever taken of what related to \r\n",
    "the victuals on the table, whether it was well or ill dressed, in or out of season, \r\n",
    "of good or bad flavor, preferable or inferior to this or that other thing \r\n",
    "of the kind, so that I was bro't up in such a perfect inattention to those matters \r\n",
    "as to be quite indifferent what kind of food was set before me, and so unobservant of \r\n",
    "it, that to this day if I am asked I can scarce tell a few hours after dinner \r\n",
    "what I dined upon. This has been a convenience to me in traveling, where my \r\n",
    "companions have been sometimes very unhappy for want of a suitable gratification \r\n",
    "of their more delicate, because better instructed, tastes and appetites.\"\"\"\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\r\n",
    "import torch\r\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\r\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens=True, max_length= 512, truncation= True, padding= 'max_length')\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1000, 2644, 25771, 1012, 1045, 1005, 1049, 2025, 2342, 2989, 1010, 2428, 1045, 1005, 1049, 2025, 1012, 2079, 2017, 2113, 1010, 1045, 2018, 1037, 3959, 2019, 3178, 3283, 1012, 1045, 3913, 2091, 2005, 1037, 4937, 1011, 18996, 1998, 1999, 2023, 3959, 2017, 1998, 1045, 1010, 18318, 8490, 1010, 2288, 2046, 1037, 9943, 5981, 2006, 2808, 1012, 2017, 3578, 2098, 2007, 7385, 1010, 7581, 16614, 2012, 2033, 1012, 1045, 12885, 11968, 11998, 2296, 7400, 1012, 2373, 1010, 1045, 2056, 1010, 1998, 2017, 1010, 27394, 2852, 1012, 3779, 1010, 2056, 1036, 3716, 2003, 2062, 2084, 5662, 2000, 2486, 999, 1005, 1998, 1045, 2056, 1010, 1036, 2092, 1010, 2852, 1012, 3779, 2036, 2056, 1010, 6203, 2879, 1010, 2008, 1000, 2002, 2003, 2053, 7968, 2158, 2008, 2097, 8046, 1037, 15855, 2005, 2019, 12503, 1012, 1005, 1000, 6293, 2007, 1996, 2543, 2386, 1010, 18318, 8490, 1012, 2035, 2842, 2003, 2852, 14644, 2100, 8488, 999, 1000, 1000, 2123, 1005, 1056, 4952, 1010, 1000, 3990, 21720, 1012, 1000, 2002, 1005, 1055, 2667, 2000, 28679, 1012, 2002, 1005, 1055, 22274, 1012, 3422, 2041, 999, 1000, 27305, 10252, 1012, 1000, 1998, 2017, 2056, 1010, 27394, 1010, 1036, 3606, 2097, 2272, 2000, 2422, 1010, 4028, 2097, 2025, 2022, 11041, 2146, 999, 1005, 1998, 1045, 6639, 1999, 2204, 17211, 1010, 1005, 2821, 2643, 1010, 2002, 8847, 2069, 1997, 2010, 3586, 999, 1005, 1998, 1036, 1996, 6548, 2064, 21893, 18919, 2005, 2010, 3800, 1012, 1005, 1998, 2017, 7581, 1010, 1005, 2023, 2287, 6732, 2488, 1997, 1037, 23880, 7966, 1010, 2084, 1997, 1037, 11689, 8237, 2063, 3002, 1999, 9866, 1005, 1055, 2082, 999, 1005, 1998, 1045, 3990, 5251, 1010, 1005, 1996, 13372, 1997, 3606, 2003, 2439, 2007, 2172, 21248, 1012, 1005, 1998, 2017, 7210, 1010, 1005, 2482, 15671, 8583, 19501, 2012, 1996, 4356, 1997, 1996, 13422, 999, 1005, 1998, 1045, 2056, 1010, 26085, 2115, 2192, 1010, 1005, 2054, 1010, 2079, 1045, 2507, 2017, 14185, 2677, 1029, 1005, 1998, 2017, 22383, 1010, 1005, 3716, 2003, 2373, 999, 1005, 1998, 1005, 1037, 11229, 2006, 1037, 5016, 1005, 1055, 4065, 1997, 1996, 6519, 20515, 1997, 1996, 2048, 999, 1005, 1998, 1045, 7680, 7583, 2026, 2217, 2039, 2007, 4678, 14262, 20693, 1999, 1010, 1005, 1996, 26272, 1997, 11094, 15495, 1037, 19240, 2005, 1037, 6947, 1010, 1037, 22047, 3372, 1997, 12034, 2401, 3351, 2005, 1037, 3500, 1997, 3007, 23019, 1010, 1998, 25763, 2004, 2019, 14721, 1010, 2003, 1999, 10280, 1999, 2149, 1010, 2720, 1012, 10380, 2854, 2320, 2056, 1012, 1005, 1000, 1045, 2228, 2017, 2089, 2066, 2000, 2113, 2242, 1997, 2010, 2711, 1998, 2839, 1012, 2002, 2018, 2019, 6581, 4552, 1997, 2303, 1010, 2001, 1997, 2690, 21120, 1010, 2021, 2092, 2275, 1010, 1998, 2200, 2844, 1025, 2002, 2001, 13749, 18595, 3560, 1010, 2071, 4009, 3653, 6916, 2135, 1010, 2001, 10571, 1037, 2210, 1999, 2189, 1010, 1998, 2018, 1037, 3154, 1010, 24820, 2376, 1010, 2061, 2008, 2043, 2002, 2209, 22728, 13281, 2006, 2010, 6710, 1998, 7042, 2007, 2389, 1010, 2004, 2002, 2823, 2106, 1999, 2019, 3944, 2044, 1996, 2449, 1997, 1996, 2154, 2001, 2058, 1010, 2009, 2001, 5186, 5993, 3085, 2000, 2963, 1012, 2002, 2018, 1037, 6228, 11067, 2205, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens= False, return_tensors= 'pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (892 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "input_id_chunks = tokens['input_ids'][0].split(510)\r\n",
    "mask_chunks = tokens['attention_mask'][0].split(510)\r\n",
    "\r\n",
    "for tensor in input_id_chunks:\r\n",
    "    print(len(tensor))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "510\n",
      "382\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "chunksize = 512\r\n",
    "\r\n",
    "input_id_chunks = list(input_id_chunks)\r\n",
    "mask_chunks = list(mask_chunks)\r\n",
    "\r\n",
    "for i in range(len(input_id_chunks)):\r\n",
    "    input_id_chunks[i] = torch.cat([torch.Tensor([101]), input_id_chunks[i], torch.Tensor([102])\r\n",
    "    ])\r\n",
    "    mask_chunks[i] = torch.cat([torch.Tensor([1]), mask_chunks[i], torch.Tensor([1])\r\n",
    "    ])\r\n",
    "\r\n",
    "    pad_len = chunksize - input_id_chunks[i].shape[0]\r\n",
    "    if pad_len>0:\r\n",
    "        input_id_chunks[i] = torch.cat([\r\n",
    "            input_id_chunks[i], torch.Tensor([0]*pad_len)\r\n",
    "            ])\r\n",
    "        mask_chunks[i] = torch.cat([\r\n",
    "            mask_chunks[i], torch.Tensor([0]*pad_len)\r\n",
    "            ])\r\n",
    "for chunk in input_id_chunks:\r\n",
    "    print(len(chunk))\r\n",
    "\r\n",
    "chunk"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "512\n",
      "512\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([  101.,  1010.,  1998.,  1010.,  2006.,  6686.,  1010.,  2001.,  2200.,\n",
       "        18801.,  1999.,  1996.,  2224.,  1997.,  2060., 14279.,  3549.,  1005.,\n",
       "         1055.,  5906.,  1025.,  2021.,  2010.,  2307.,  8012.,  3913.,  1999.,\n",
       "         1037.,  2614.,  4824.,  1998.,  5024.,  8689.,  1999., 10975., 12672.,\n",
       "        19909.,  5609.,  1010.,  2119.,  1999.,  2797.,  1998.,  2270.,  2243.,\n",
       "         3821.,  1012.,  1999.,  1996.,  3732.,  1010.,  5262.,  1010.,  2002.,\n",
       "         2001.,  2196.,  4846.,  1010.,  1996.,  3365.,  2155.,  2002.,  2018.,\n",
       "         2000., 16957.,  1998.,  1996., 11195.,  2791.,  1997.,  2010.,  6214.,\n",
       "         4363.,  2032.,  2485.,  2000.,  2010.,  3119.,  1025.,  2021.,  1045.,\n",
       "         3342.,  2092.,  2010.,  2108.,  4703.,  4716.,  2011.,  2877.,  2111.,\n",
       "         1010.,  2040., 17535.,  2032.,  2005.,  2010.,  5448.,  1999.,  3821.,\n",
       "         1997.,  1996.,  2237.,  2030.,  1997.,  1996.,  2277.,  2002.,  6272.,\n",
       "         2000.,  1010.,  1998.,  3662.,  1037.,  2204.,  3066.,  1997.,  4847.,\n",
       "         2005.,  2010.,  8689.,  1998.,  6040.,  1024.,  2002.,  2001.,  2036.,\n",
       "         2172., 17535.,  2011.,  2797.,  5381.,  2055.,  2037.,  3821.,  2043.,\n",
       "         2151.,  7669.,  4158.,  1010.,  1998.,  4703.,  4217.,  2019., 12098.,\n",
       "        16313., 16259.,  2090., 27481.,  2075.,  4243.,  1012.,  2012.,  2010.,\n",
       "         2795.,  2002.,  4669.,  2000.,  2031.,  1010.,  2004.,  2411.,  2004.,\n",
       "         2002.,  2071.,  1010.,  2070., 21082.,  2767.,  2030., 11429.,  2000.,\n",
       "        23705.,  2007.,  1010.,  1998.,  2467.,  2165.,  2729.,  2000.,  2707.,\n",
       "         2070., 13749., 18595.,  3560.,  2030.,  6179.,  8476.,  2005., 15152.,\n",
       "         1010.,  2029.,  2453.,  7166.,  2000.,  5335.,  1996.,  9273.,  1997.,\n",
       "         2010.,  2336.,  1012.,  2011.,  2023.,  2965.,  2002.,  2357.,  2256.,\n",
       "         3086.,  2000.,  2054.,  2001.,  2204.,  1010.,  2074.,  1010.,  1998.,\n",
       "        10975., 12672.,  3372.,  1999.,  1996.,  6204.,  1997.,  2166.,  1025.,\n",
       "         1998.,  2210.,  2030.,  2053.,  5060.,  2001.,  2412.,  2579.,  1997.,\n",
       "         2054.,  3141.,  2000.,  1996., 10967., 26302.,  4877.,  2006.,  1996.,\n",
       "         2795.,  1010.,  3251.,  2009.,  2001.,  2092.,  2030.,  5665.,  5102.,\n",
       "         1010.,  1999.,  2030.,  2041.,  1997.,  2161.,  1010.,  1997.,  2204.,\n",
       "         2030.,  2919., 14894.,  1010.,  9544.,  3085.,  2030., 14092.,  2000.,\n",
       "         2023.,  2030.,  2008.,  2060.,  2518.,  1997.,  1996.,  2785.,  1010.,\n",
       "         2061.,  2008.,  1045.,  2001., 22953.,  1005.,  1056.,  2039.,  1999.,\n",
       "         2107.,  1037.,  3819., 27118., 25970.,  3508.,  2000.,  2216.,  5609.,\n",
       "         2004.,  2000.,  2022.,  3243., 24436.,  2054.,  2785.,  1997.,  2833.,\n",
       "         2001.,  2275.,  2077.,  2033.,  1010.,  1998.,  2061., 27776.,  5910.,\n",
       "         2121., 18941.,  1997.,  2009.,  1010.,  2008.,  2000.,  2023.,  2154.,\n",
       "         2065.,  1045.,  2572.,  2356.,  1045.,  2064., 18782.,  2425.,  1037.,\n",
       "         2261.,  2847.,  2044.,  4596.,  2054.,  1045., 11586.,  2098.,  2588.,\n",
       "         1012.,  2023.,  2038.,  2042.,  1037., 15106.,  2000.,  2033.,  1999.,\n",
       "         7118.,  1010.,  2073.,  2026., 11946.,  2031.,  2042.,  2823.,  2200.,\n",
       "        12511.,  2005.,  2215.,  1997.,  1037.,  7218., 24665., 10450., 10803.,\n",
       "         1997.,  2037.,  2062., 10059.,  1010.,  2138.,  2488., 10290.,  1010.,\n",
       "        16958.,  1998., 18923.,  2015.,  1012.,   102.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "input_ids = torch.stack(input_id_chunks)\r\n",
    "attention_mask = torch.stack(mask_chunks)\r\n",
    "\r\n",
    "input_dict = {\r\n",
    "    'input_ids' : input_ids.long(),\r\n",
    "    'attention_mask' : attention_mask.int(),\r\n",
    "\r\n",
    "}\r\n",
    "\r\n",
    "input_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1000,  2644,  ..., 11067,  2205,   102],\n",
       "         [  101,  1010,  1998,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "outputs = model(**input_dict)\r\n",
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9745, -0.2582,  1.8004],\n",
       "        [-1.0032, -0.4195,  1.8410]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "probs = torch.nn.functional.softmax(outputs[0], dim =-1)\r\n",
    "\r\n",
    "probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0524, 0.1073, 0.8403],\n",
       "        [0.0500, 0.0897, 0.8602]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "mean = probs.mean(dim =0)\r\n",
    "torch.argmax(mean).item()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "05aff013e39b780ca07193452c9ca98a8e895dc233a98a9e203a3c03f5b2b4ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}